{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import datetime\n",
    "from gym import wrappers\n",
    "from particle_envs.make_env import make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, num_states, hidden_units, num_actions):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
    "        self.hidden_layers = []\n",
    "        for output in hidden_units:\n",
    "            self.hidden_layers.append(Dense(output, activation='tanh', kernel_initializer='RandomNormal'))\n",
    "        self.output_layer = Dense(num_actions, activation='linear', kernel_initializer='RandomNormal')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        z = self.input_layer(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        output = self.output_layer(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.num_actions = num_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = tf.optimizers.Adam(lr)\n",
    "        self.gamma = gamma\n",
    "        self.model = MyModel(num_states, hidden_units, num_actions)\n",
    "        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}  # done (bool): is current state terminal\n",
    "        self.max_experiences = max_experiences\n",
    "        self.min_experiences = min_experiences\n",
    "         \n",
    "    def predict(self, inputs):\n",
    "        return self.model(np.atleast_2d(inputs.astype('float32')))\n",
    "    \n",
    "    def train(self, TargetNet):\n",
    "        if len(self.experience['s']) < self.min_experiences:\n",
    "            return 0, 0\n",
    "        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n",
    "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        value_next = np.max(TargetNet.predict(states_next), axis=1)\n",
    "        actual_values = np.where(dones, rewards, rewards + self.gamma*value_next)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            selected_action_values = tf.math.reduce_sum(\n",
    "                self.predict(states) * actions)\n",
    "            loss = tf.math.reduce_mean(tf.square(actual_values - selected_action_values))\n",
    "    \n",
    "        variables = self.model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss, selected_action_values\n",
    "        \n",
    "    def get_action(self, states, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.predict(np.atleast_2d(states))[0])\n",
    "\n",
    "\n",
    "    def add_experience(self, exp):\n",
    "        if len(self.experience['s']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "        for key, value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "\n",
    "    def copy_weights(self, TrainNet):\n",
    "        variables1 = self.model.trainable_variables\n",
    "        variables2 = TrainNet.model.trainable_variables\n",
    "        for v1, v2 in zip(variables1, variables2):\n",
    "            v1.assign(v2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()[0]\n",
    "    losses = list()\n",
    "    episode_length = 200\n",
    "    for i in range(episode_length):\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        # Convert action to one-hot\n",
    "        action = [np.identity(env.action_space[0].n, dtype=int)[action]]\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        observations, reward, done = observations[0], reward[0], done[0]\n",
    "        rewards += reward\n",
    "#         if done:\n",
    "#             reward = -200\n",
    "#             env.reset()\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        loss, selected_action_values = TrainNet.train(TargetNet)\n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    return rewards, np.mean(losses), np.mean(selected_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(env, TrainNet):\n",
    "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observation, 0)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = make_env('simple')\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "    num_states = len(env.observation_space[0].sample())\n",
    "    num_actions = env.action_space[0].n\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    batch_size = 32\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    N = 10000\n",
    "    total_rewards = np.empty(N)\n",
    "    epsilon = 0.99\n",
    "    decay = 0.9999\n",
    "    min_epsilon = 0.1\n",
    "    for n in range(N):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward, losses, action_values_avg = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "            tf.summary.scalar('average loss)', losses, step=n)\n",
    "            tf.summary.scalar('average Q value', action_values_avg, step=n)\n",
    "        if True:  # n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"epsilon:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
    "                  \"episode loss: \", losses)\n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\microsoft visual studio\\shared\\python37_64\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: -2351.87633023031 epsilon: 0.989901 avg reward (last 100): -2351.87633023031 episode loss:  8305.412998809814\n",
      "episode: 1 episode reward: -647.5958264123548 epsilon: 0.9898020099 avg reward (last 100): -1499.7360783213323 episode loss:  127.67553\n",
      "episode: 2 episode reward: -1190.1174313025242 epsilon: 0.98970302969901 avg reward (last 100): -1396.5298626483964 episode loss:  64.92725\n",
      "episode: 3 episode reward: -2470.472923902548 epsilon: 0.9896040593960401 avg reward (last 100): -1665.0156279619341 episode loss:  108.05304\n",
      "episode: 4 episode reward: -105.87776936564642 epsilon: 0.9895050989901005 avg reward (last 100): -1353.1880562426766 episode loss:  107.6651\n",
      "episode: 5 episode reward: -305.3991402700159 epsilon: 0.9894061484802015 avg reward (last 100): -1178.5565702472331 episode loss:  47.257256\n",
      "episode: 6 episode reward: -303.20406160689305 epsilon: 0.9893072078653534 avg reward (last 100): -1053.5062118700419 episode loss:  41.080708\n",
      "episode: 7 episode reward: -771.738098680684 epsilon: 0.9892082771445669 avg reward (last 100): -1018.285197721372 episode loss:  38.214706\n",
      "episode: 8 episode reward: -514.932899056798 epsilon: 0.9891093563168525 avg reward (last 100): -962.3571645364193 episode loss:  41.437046\n",
      "episode: 9 episode reward: -293.2375112947892 epsilon: 0.9890104453812208 avg reward (last 100): -895.4451992122562 episode loss:  36.500866\n",
      "episode: 10 episode reward: -196.27679586536712 epsilon: 0.9889115443366827 avg reward (last 100): -831.88443527163 episode loss:  34.41211\n",
      "episode: 11 episode reward: -633.0739686709579 epsilon: 0.9888126531822491 avg reward (last 100): -815.3168963882407 episode loss:  80.176056\n",
      "episode: 12 episode reward: -1010.3334030686933 epsilon: 0.9887137719169309 avg reward (last 100): -830.3181661328908 episode loss:  38.345745\n",
      "episode: 13 episode reward: -782.5302371487891 epsilon: 0.9886149005397392 avg reward (last 100): -826.9047426340264 episode loss:  29.247828\n",
      "episode: 14 episode reward: -262.1745596851145 epsilon: 0.9885160390496852 avg reward (last 100): -789.2560637707655 episode loss:  26.988874\n",
      "episode: 15 episode reward: -658.1140617994565 epsilon: 0.9884171874457802 avg reward (last 100): -781.0596886475588 episode loss:  25.033907\n",
      "episode: 16 episode reward: -843.7547936088197 epsilon: 0.9883183457270357 avg reward (last 100): -784.7476359982212 episode loss:  29.356478\n",
      "episode: 17 episode reward: -331.6886094781593 epsilon: 0.988219513892463 avg reward (last 100): -759.5776900804399 episode loss:  25.938759\n",
      "episode: 18 episode reward: -1619.727532106451 epsilon: 0.9881206919410738 avg reward (last 100): -804.8487343975985 episode loss:  27.066072\n",
      "episode: 19 episode reward: -2002.6914356564876 epsilon: 0.9880218798718797 avg reward (last 100): -864.7408694605429 episode loss:  32.76261\n",
      "episode: 20 episode reward: -1128.622559470241 epsilon: 0.9879230776838925 avg reward (last 100): -877.3066642229094 episode loss:  69.858444\n",
      "episode: 21 episode reward: -645.6776843570985 epsilon: 0.9878242853761242 avg reward (last 100): -866.7780742290089 episode loss:  390277.62\n",
      "episode: 22 episode reward: -72.80835753992285 epsilon: 0.9877255029475867 avg reward (last 100): -832.257651764266 episode loss:  272796.75\n",
      "episode: 23 episode reward: -206.12331637992668 epsilon: 0.9876267303972919 avg reward (last 100): -806.168721123252 episode loss:  83735.27\n",
      "episode: 24 episode reward: -1153.5744414977983 epsilon: 0.9875279677242521 avg reward (last 100): -820.0649499382339 episode loss:  15657.012\n",
      "episode: 25 episode reward: -545.0211785961939 epsilon: 0.9874292149274797 avg reward (last 100): -809.4863433481555 episode loss:  6712.1875\n",
      "episode: 26 episode reward: -169.84200672171218 epsilon: 0.987330472005987 avg reward (last 100): -785.7958123619909 episode loss:  16401.795\n",
      "episode: 27 episode reward: -644.565607799742 epsilon: 0.9872317389587865 avg reward (last 100): -780.7518764847676 episode loss:  12651.544\n",
      "episode: 28 episode reward: -552.7016321972942 epsilon: 0.9871330157848905 avg reward (last 100): -772.8880749576135 episode loss:  38892.99\n",
      "episode: 29 episode reward: -1363.5548051858705 epsilon: 0.987034302483312 avg reward (last 100): -792.576965965222 episode loss:  41657.78\n",
      "episode: 30 episode reward: -384.6096243509024 epsilon: 0.9869355990530637 avg reward (last 100): -779.4167291389537 episode loss:  19504.672\n",
      "episode: 31 episode reward: -521.0675280477599 epsilon: 0.9868369054931584 avg reward (last 100): -771.3433166048538 episode loss:  79492.13\n",
      "episode: 32 episode reward: -672.8441285931291 epsilon: 0.9867382218026091 avg reward (last 100): -768.3584927257107 episode loss:  60498.86\n",
      "episode: 33 episode reward: -335.50653515861615 epsilon: 0.9866395479804289 avg reward (last 100): -755.6275527972667 episode loss:  31513.74\n",
      "episode: 34 episode reward: -1143.3111428607945 epsilon: 0.9865408840256308 avg reward (last 100): -766.7042267990819 episode loss:  4715.3374\n",
      "episode: 35 episode reward: -90.92865490526029 epsilon: 0.9864422299372282 avg reward (last 100): -747.9326831353645 episode loss:  6169.1626\n",
      "episode: 36 episode reward: -535.0603480431347 epsilon: 0.9863435857142345 avg reward (last 100): -742.1793767815204 episode loss:  13538.145\n",
      "episode: 37 episode reward: -589.3573808227098 epsilon: 0.9862449513556631 avg reward (last 100): -738.1577453089202 episode loss:  209564.64\n",
      "episode: 38 episode reward: -198.59222119388355 epsilon: 0.9861463268605276 avg reward (last 100): -724.3227318700731 episode loss:  52004.43\n",
      "episode: 39 episode reward: -489.7500357546094 epsilon: 0.9860477122278416 avg reward (last 100): -718.4584144671865 episode loss:  23964.164\n",
      "episode: 40 episode reward: -1182.4332291167086 epsilon: 0.9859491074566188 avg reward (last 100): -729.7748733610773 episode loss:  44208.605\n",
      "episode: 41 episode reward: -120.0237682189505 epsilon: 0.9858505125458732 avg reward (last 100): -715.2569899053124 episode loss:  43341.73\n",
      "episode: 42 episode reward: -199.32516251487695 epsilon: 0.9857519274946186 avg reward (last 100): -703.2585753148371 episode loss:  4140.8525\n",
      "episode: 43 episode reward: -505.31090787536453 epsilon: 0.9856533523018691 avg reward (last 100): -698.7597646912128 episode loss:  80862.05\n",
      "episode: 44 episode reward: -411.6648513158332 epsilon: 0.9855547869666389 avg reward (last 100): -692.3798777273155 episode loss:  10645.42\n",
      "episode: 45 episode reward: -424.34067113428745 epsilon: 0.9854562314879423 avg reward (last 100): -686.5529384535539 episode loss:  35598.99\n",
      "episode: 46 episode reward: -154.81919353961078 epsilon: 0.9853576858647934 avg reward (last 100): -675.2394545192147 episode loss:  113222.64\n",
      "episode: 47 episode reward: -2204.474512218648 epsilon: 0.985259150096207 avg reward (last 100): -707.0985182212862 episode loss:  9032.521\n",
      "episode: 48 episode reward: -1214.0937136462478 epsilon: 0.9851606241811974 avg reward (last 100): -717.4453589442446 episode loss:  9034.444\n",
      "episode: 49 episode reward: -359.0935274918806 epsilon: 0.9850621081187794 avg reward (last 100): -710.2783223151973 episode loss:  3132.6875\n",
      "episode: 50 episode reward: -143.50884790888392 epsilon: 0.9849636019079675 avg reward (last 100): -699.1651953660538 episode loss:  8324.253\n",
      "episode: 51 episode reward: -367.4099106269125 epsilon: 0.9848651055477767 avg reward (last 100): -692.7852860441473 episode loss:  7644.07\n",
      "episode: 52 episode reward: -935.3024461545322 epsilon: 0.984766619037222 avg reward (last 100): -697.3610815179281 episode loss:  8042.6187\n",
      "episode: 53 episode reward: -628.049071287104 epsilon: 0.9846681423753183 avg reward (last 100): -696.077525772913 episode loss:  25808.807\n",
      "episode: 54 episode reward: -269.5352558358987 epsilon: 0.9845696755610808 avg reward (last 100): -688.3222117740581 episode loss:  52896.39\n",
      "episode: 55 episode reward: -2191.7135608431536 epsilon: 0.9844712185935247 avg reward (last 100): -715.1684858645777 episode loss:  15446.572\n",
      "episode: 56 episode reward: -311.3314888967455 epsilon: 0.9843727714716654 avg reward (last 100): -708.0836262686508 episode loss:  46563.27\n",
      "episode: 57 episode reward: -1344.5968213431224 epsilon: 0.9842743341945182 avg reward (last 100): -719.0579917009693 episode loss:  43111.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 58 episode reward: -347.6349974948461 epsilon: 0.9841759067610987 avg reward (last 100): -712.7626867144248 episode loss:  500109.16\n",
      "episode: 59 episode reward: -559.7136222204308 epsilon: 0.9840774891704226 avg reward (last 100): -710.2118689728582 episode loss:  17065.604\n",
      "episode: 60 episode reward: -428.8934599013523 epsilon: 0.9839790814215056 avg reward (last 100): -705.6000917749647 episode loss:  6173.777\n",
      "episode: 61 episode reward: -556.5338238425292 epsilon: 0.9838806835133634 avg reward (last 100): -703.1957971308932 episode loss:  3552.835\n",
      "episode: 62 episode reward: -2149.9601510146526 epsilon: 0.9837822954450122 avg reward (last 100): -726.1603106846036 episode loss:  3507.86\n",
      "episode: 63 episode reward: -533.8637285024191 epsilon: 0.9836839172154677 avg reward (last 100): -723.155676588007 episode loss:  2910.9897\n",
      "episode: 64 episode reward: -719.60263152219 epsilon: 0.9835855488237462 avg reward (last 100): -723.1010143562253 episode loss:  4726.5645\n",
      "episode: 65 episode reward: -667.8227489409317 epsilon: 0.9834871902688638 avg reward (last 100): -722.2634648802359 episode loss:  15918.543\n",
      "episode: 66 episode reward: -233.62803728602512 epsilon: 0.9833888415498369 avg reward (last 100): -714.9703987967404 episode loss:  18959.748\n",
      "episode: 67 episode reward: -458.0481120878223 epsilon: 0.9832905026656819 avg reward (last 100): -711.1921298745503 episode loss:  4005.5823\n",
      "episode: 68 episode reward: -1070.686004296081 epsilon: 0.9831921736154153 avg reward (last 100): -716.4021860255871 episode loss:  3071.55\n",
      "episode: 69 episode reward: -273.13015900269045 epsilon: 0.9830938543980539 avg reward (last 100): -710.0697284966886 episode loss:  38208.94\n",
      "episode: 70 episode reward: -918.3358197993541 epsilon: 0.9829955450126141 avg reward (last 100): -713.0030537263036 episode loss:  24688.967\n",
      "episode: 71 episode reward: -334.96390284056986 epsilon: 0.9828972454581129 avg reward (last 100): -707.7525099640015 episode loss:  23825.195\n",
      "episode: 72 episode reward: -563.4052113724164 epsilon: 0.982798955733567 avg reward (last 100): -705.7751497093224 episode loss:  12507.117\n",
      "episode: 73 episode reward: -615.4332388435245 epsilon: 0.9827006758379937 avg reward (last 100): -704.5543130760007 episode loss:  22457.5\n",
      "episode: 74 episode reward: -2154.817643418799 epsilon: 0.9826024057704099 avg reward (last 100): -723.8911574805713 episode loss:  23230.28\n",
      "episode: 75 episode reward: -433.77086511319146 epsilon: 0.9825041455298329 avg reward (last 100): -720.0737852125794 episode loss:  44919.023\n",
      "episode: 76 episode reward: -1115.444370201677 epsilon: 0.9824058951152799 avg reward (last 100): -725.2084681345158 episode loss:  65222.734\n",
      "episode: 77 episode reward: -155.59074467025303 epsilon: 0.9823076545257684 avg reward (last 100): -717.905676808051 episode loss:  133042.08\n",
      "episode: 78 episode reward: -368.43891654265406 epsilon: 0.9822094237603158 avg reward (last 100): -713.4820469312738 episode loss:  19625.22\n",
      "episode: 79 episode reward: -2258.637301673624 epsilon: 0.9821112028179398 avg reward (last 100): -732.7964876155531 episode loss:  8124.9487\n",
      "episode: 80 episode reward: -148.15953521077486 epsilon: 0.9820129916976581 avg reward (last 100): -725.5787474624077 episode loss:  49846.977\n",
      "episode: 81 episode reward: -1606.919003777505 epsilon: 0.9819147903984884 avg reward (last 100): -736.3267993686893 episode loss:  23243.176\n",
      "episode: 82 episode reward: -1226.1950041193497 epsilon: 0.9818165989194485 avg reward (last 100): -742.2288259319503 episode loss:  13526.457\n",
      "episode: 83 episode reward: -1152.554411419984 epsilon: 0.9817184172595566 avg reward (last 100): -747.1136543306175 episode loss:  14088.928\n",
      "episode: 84 episode reward: -1602.250893916856 epsilon: 0.9816202454178307 avg reward (last 100): -757.1740924433967 episode loss:  36016.73\n",
      "episode: 85 episode reward: -163.68496444538906 epsilon: 0.9815220833932888 avg reward (last 100): -750.2730560713269 episode loss:  65855.15\n",
      "episode: 86 episode reward: -509.9084664490064 epsilon: 0.9814239311849495 avg reward (last 100): -747.5102446963576 episode loss:  131723.86\n",
      "episode: 87 episode reward: -588.5957979013464 epsilon: 0.9813257887918311 avg reward (last 100): -745.7043987100508 episode loss:  111937.92\n",
      "episode: 88 episode reward: -435.4989920415501 epsilon: 0.9812276562129519 avg reward (last 100): -742.2189447025396 episode loss:  19454.365\n",
      "episode: 89 episode reward: -875.6623634801359 epsilon: 0.9811295334473307 avg reward (last 100): -743.701649355624 episode loss:  18655.182\n",
      "episode: 90 episode reward: -491.31509422304987 epsilon: 0.981031420493986 avg reward (last 100): -740.9281707277936 episode loss:  12626.803\n",
      "episode: 91 episode reward: -1394.0928316591032 epsilon: 0.9809333173519366 avg reward (last 100): -748.0277866074817 episode loss:  4202.3296\n",
      "episode: 92 episode reward: -886.8051618547905 epsilon: 0.9808352240202014 avg reward (last 100): -749.5200164488507 episode loss:  56674.04\n",
      "episode: 93 episode reward: -345.2526475501068 epsilon: 0.9807371404977994 avg reward (last 100): -745.2192997584385 episode loss:  75400.94\n",
      "episode: 94 episode reward: -576.1593933438331 epsilon: 0.9806390667837496 avg reward (last 100): -743.4397217961794 episode loss:  59717.54\n",
      "episode: 95 episode reward: -442.25147594102486 epsilon: 0.9805410028770712 avg reward (last 100): -740.3023442351881 episode loss:  45101.844\n",
      "episode: 96 episode reward: -1046.5691440010435 epsilon: 0.9804429487767835 avg reward (last 100): -743.4597339234958 episode loss:  19724.488\n",
      "episode: 97 episode reward: -1368.124573294014 epsilon: 0.9803449044819058 avg reward (last 100): -749.8338649374807 episode loss:  12468.559\n",
      "episode: 98 episode reward: -1074.2192886982227 epsilon: 0.9802468699914576 avg reward (last 100): -753.1104853795084 episode loss:  11484.232\n",
      "episode: 99 episode reward: -351.2543484743463 epsilon: 0.9801488453044584 avg reward (last 100): -749.0919240104569 episode loss:  9151.586\n",
      "episode: 100 episode reward: -1103.6092586188277 epsilon: 0.980050830419928 avg reward (last 100): -752.6019966303418 episode loss:  62792.33\n",
      "episode: 101 episode reward: -371.5071082514578 epsilon: 0.979952825336886 avg reward (last 100): -732.9943805711453 episode loss:  108997.86\n",
      "episode: 102 episode reward: -1888.604408421564 epsilon: 0.9798548300543524 avg reward (last 100): -745.2815942544048 episode loss:  90209.86\n",
      "episode: 103 episode reward: -741.1097947398904 epsilon: 0.9797568445713469 avg reward (last 100): -740.8359740904184 episode loss:  30741.564\n",
      "episode: 104 episode reward: -1144.0895604955488 epsilon: 0.9796588688868898 avg reward (last 100): -727.703465541834 episode loss:  12727.025\n",
      "episode: 105 episode reward: -286.98500075460413 epsilon: 0.9795609030000011 avg reward (last 100): -729.4966064466753 episode loss:  26822.285\n",
      "episode: 106 episode reward: -2360.4199305464663 epsilon: 0.9794629469097011 avg reward (last 100): -749.8433469444619 episode loss:  39261.457\n",
      "episode: 107 episode reward: -531.1683347786382 epsilon: 0.9793650006150102 avg reward (last 100): -752.1004189560633 episode loss:  150868.08\n",
      "episode: 108 episode reward: -264.1911428840814 epsilon: 0.9792670641149487 avg reward (last 100): -747.0752015719384 episode loss:  21991.5\n",
      "episode: 109 episode reward: -453.61409351109035 epsilon: 0.9791691374085372 avg reward (last 100): -746.4680846853472 episode loss:  8215.032\n",
      "episode: 110 episode reward: -265.29885324082846 epsilon: 0.9790712204947963 avg reward (last 100): -746.1914643085754 episode loss:  5528.6963\n",
      "episode: 111 episode reward: -1011.085805598554 epsilon: 0.9789733133727468 avg reward (last 100): -754.2588802465277 episode loss:  4781.399\n",
      "episode: 112 episode reward: -351.4953513650773 epsilon: 0.9788754160414096 avg reward (last 100): -751.4709731444896 episode loss:  51045.555\n",
      "episode: 113 episode reward: -1480.1722199335636 epsilon: 0.9787775284998055 avg reward (last 100): -756.1228426183992 episode loss:  74729.45\n",
      "episode: 114 episode reward: -501.905312358281 epsilon: 0.9786796507469555 avg reward (last 100): -753.344378016513 episode loss:  31529.23\n",
      "episode: 115 episode reward: -490.3575752725077 epsilon: 0.9785817827818808 avg reward (last 100): -755.6036157946058 episode loss:  43898.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 116 episode reward: -240.95243512497453 epsilon: 0.9784839246036026 avg reward (last 100): -751.4733026592149 episode loss:  17089.043\n",
      "episode: 117 episode reward: -798.3238900213445 epsilon: 0.9783860762111423 avg reward (last 100): -751.0234917326062 episode loss:  21784.418\n",
      "episode: 118 episode reward: -2366.5598113345413 epsilon: 0.9782882376035211 avg reward (last 100): -771.1707313549467 episode loss:  46893.07\n",
      "episode: 119 episode reward: -101.26876202032082 epsilon: 0.9781904087797608 avg reward (last 100): -756.136486106569 episode loss:  51557.71\n",
      "episode: 120 episode reward: -454.90507641180085 epsilon: 0.9780925897388829 avg reward (last 100): -740.811868688305 episode loss:  16288.855\n",
      "episode: 121 episode reward: -591.8490304296564 epsilon: 0.9779947804799091 avg reward (last 100): -735.4972792918635 episode loss:  148630.9\n",
      "episode: 122 episode reward: -1209.1377645877562 epsilon: 0.9778969810018611 avg reward (last 100): -741.0760919674146 episode loss:  59248.37\n",
      "episode: 123 episode reward: -376.258249785801 epsilon: 0.9777991913037609 avg reward (last 100): -744.0805463460863 episode loss:  30159.936\n",
      "episode: 124 episode reward: -1664.709151156591 epsilon: 0.9777014113846305 avg reward (last 100): -758.5219902547663 episode loss:  13149.651\n",
      "episode: 125 episode reward: -320.98669764226577 epsilon: 0.9776036412434921 avg reward (last 100): -750.2785472462958 episode loss:  11881.42\n",
      "episode: 126 episode reward: -417.56136577936655 epsilon: 0.9775058808793677 avg reward (last 100): -749.0165689015746 episode loss:  15914.635\n",
      "episode: 127 episode reward: -2719.9419680745696 epsilon: 0.9774081302912798 avg reward (last 100): -774.2650833704148 episode loss:  31894.07\n",
      "episode: 128 episode reward: -539.6740774522334 epsilon: 0.9773103894782507 avg reward (last 100): -773.2265533669743 episode loss:  35029.156\n",
      "episode: 129 episode reward: -1155.2592036950914 epsilon: 0.9772126584393028 avg reward (last 100): -779.1924699164574 episode loss:  66727.05\n",
      "episode: 130 episode reward: -336.2783926004065 epsilon: 0.9771149371734589 avg reward (last 100): -769.0214163265023 episode loss:  16745.54\n",
      "episode: 131 episode reward: -1298.1672206893268 epsilon: 0.9770172256797416 avg reward (last 100): -778.0665410427242 episode loss:  24805.51\n",
      "episode: 132 episode reward: -897.7860595328016 epsilon: 0.9769195239571736 avg reward (last 100): -781.7964274930711 episode loss:  50518.94\n",
      "episode: 133 episode reward: -701.7577811864206 epsilon: 0.9768218320047779 avg reward (last 100): -782.0827012811236 episode loss:  16160.253\n",
      "episode: 134 episode reward: -841.9243608079895 epsilon: 0.9767241498215774 avg reward (last 100): -787.0967391588401 episode loss:  7518.91\n",
      "episode: 135 episode reward: -1265.5477464356636 epsilon: 0.9766264774065953 avg reward (last 100): -788.3070025605714 episode loss:  43141.56\n",
      "episode: 136 episode reward: -1206.3737030497382 epsilon: 0.9765288147588546 avg reward (last 100): -799.3510129382395 episode loss:  80456.76\n",
      "episode: 137 episode reward: -125.17511581625112 epsilon: 0.9764311618773787 avg reward (last 100): -795.292743312231 episode loss:  26393.766\n",
      "episode: 138 episode reward: -2562.1126007776907 epsilon: 0.9763335187611909 avg reward (last 100): -814.8249732127754 episode loss:  228843.06\n",
      "episode: 139 episode reward: -473.34198026167655 epsilon: 0.9762358854093148 avg reward (last 100): -817.5452678570108 episode loss:  18339.422\n",
      "episode: 140 episode reward: -443.76823314735094 epsilon: 0.9761382618207739 avg reward (last 100): -817.0900024846617 episode loss:  57309.44\n",
      "episode: 141 episode reward: -1369.394506800387 epsilon: 0.9760406479945918 avg reward (last 100): -818.941104243906 episode loss:  18091.936\n",
      "episode: 142 episode reward: -565.4747837200172 epsilon: 0.9759430439297924 avg reward (last 100): -823.3515103379759 episode loss:  29099.568\n",
      "episode: 143 episode reward: -399.2894178945413 epsilon: 0.9758454496253994 avg reward (last 100): -825.3313544506458 episode loss:  29621.36\n",
      "episode: 144 episode reward: -159.8795546751496 epsilon: 0.9757478650804369 avg reward (last 100): -821.911242042723 episode loss:  57996.39\n",
      "episode: 145 episode reward: -2217.5574286239416 epsilon: 0.9756502902939288 avg reward (last 100): -839.7913665705261 episode loss:  78861.4\n",
      "episode: 146 episode reward: -491.27904151012723 epsilon: 0.9755527252648994 avg reward (last 100): -840.4541227128611 episode loss:  77175.18\n",
      "episode: 147 episode reward: -499.4699620918703 epsilon: 0.9754551699923729 avg reward (last 100): -843.8665065599134 episode loss:  13362.022\n",
      "episode: 148 episode reward: -120.65928608758333 epsilon: 0.9753576244753737 avg reward (last 100): -823.2346726378236 episode loss:  10722.414\n",
      "episode: 149 episode reward: -505.03772391691416 epsilon: 0.9752600887129261 avg reward (last 100): -816.2143163038699 episode loss:  9740.262\n",
      "episode: 150 episode reward: -200.30339053677773 epsilon: 0.9751625627040549 avg reward (last 100): -814.6421367300569 episode loss:  12284.56\n",
      "episode: 151 episode reward: -2190.469298668581 epsilon: 0.9750650464477845 avg reward (last 100): -834.9090718860934 episode loss:  25487.54\n",
      "episode: 152 episode reward: -1770.453846188586 epsilon: 0.9749675399431397 avg reward (last 100): -848.8005960005654 episode loss:  23354.182\n",
      "episode: 153 episode reward: -1001.7729260861349 epsilon: 0.9748700431891454 avg reward (last 100): -849.4587195642448 episode loss:  70750.64\n",
      "episode: 154 episode reward: -1096.5863621285769 epsilon: 0.9747725561848265 avg reward (last 100): -854.0977026418831 episode loss:  92465.04\n",
      "episode: 155 episode reward: -122.7508599943076 epsilon: 0.974675078929208 avg reward (last 100): -852.6443917919663 episode loss:  35075.047\n",
      "episode: 156 episode reward: -437.85760177954336 epsilon: 0.9745776114213152 avg reward (last 100): -835.279481306188 episode loss:  56650.91\n",
      "episode: 157 episode reward: -180.94116477883824 epsilon: 0.974480153660173 avg reward (last 100): -833.9884879980899 episode loss:  23848.71\n",
      "episode: 158 episode reward: -576.3552051205797 epsilon: 0.974382705644807 avg reward (last 100): -826.3821353622232 episode loss:  13157.803\n",
      "episode: 159 episode reward: -595.3850883071281 epsilon: 0.9742852673742425 avg reward (last 100): -828.8351065583843 episode loss:  36895.273\n",
      "episode: 160 episode reward: -756.5090526099865 epsilon: 0.974187838847505 avg reward (last 100): -830.7835761662017 episode loss:  20118.156\n",
      "episode: 161 episode reward: -823.2184160158213 epsilon: 0.9740904200636203 avg reward (last 100): -834.6877836524834 episode loss:  15533.108\n",
      "episode: 162 episode reward: -1540.578208947723 epsilon: 0.973993011021614 avg reward (last 100): -844.4307973663963 episode loss:  36827.62\n",
      "episode: 163 episode reward: -581.8951117049091 epsilon: 0.9738956117205118 avg reward (last 100): -828.905400937587 episode loss:  30241.78\n",
      "episode: 164 episode reward: -329.51914437522015 epsilon: 0.9737982221593398 avg reward (last 100): -826.8821872333575 episode loss:  152780.73\n",
      "episode: 165 episode reward: -121.36655104231649 epsilon: 0.9737008423371238 avg reward (last 100): -820.9590577236556 episode loss:  53576.656\n",
      "episode: 166 episode reward: -399.86971472315287 epsilon: 0.9736034722528901 avg reward (last 100): -818.306057384866 episode loss:  24305.395\n",
      "episode: 167 episode reward: -761.5967100580177 epsilon: 0.9735061119056648 avg reward (last 100): -823.5334699865688 episode loss:  24330.57\n",
      "episode: 168 episode reward: -429.25126231757105 epsilon: 0.9734087612944743 avg reward (last 100): -823.2483526621108 episode loss:  13354.14\n",
      "episode: 169 episode reward: -1010.7904300169533 epsilon: 0.9733114204183448 avg reward (last 100): -822.6553271741985 episode loss:  20043.8\n",
      "episode: 170 episode reward: -433.40539416887515 epsilon: 0.973214089276303 avg reward (last 100): -824.2422106906954 episode loss:  17634.355\n",
      "episode: 171 episode reward: -387.6239512742874 epsilon: 0.9731167678673754 avg reward (last 100): -818.9876377350016 episode loss:  83759.66\n",
      "episode: 172 episode reward: -687.6539796195584 epsilon: 0.9730194561905887 avg reward (last 100): -822.4796186932094 episode loss:  23816.977\n",
      "episode: 173 episode reward: -196.78789690727834 epsilon: 0.9729221542449696 avg reward (last 100): -818.8497442925645 episode loss:  80312.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 174 episode reward: -51.077051056809545 epsilon: 0.9728248620295451 avg reward (last 100): -813.2620592649732 episode loss:  28855.922\n",
      "episode: 175 episode reward: -182.16674009318507 epsilon: 0.9727275795433421 avg reward (last 100): -793.7308622023437 episode loss:  52334.246\n",
      "episode: 176 episode reward: -654.7284875258223 epsilon: 0.9726303067853878 avg reward (last 100): -795.9185614341517 episode loss:  12923.022\n",
      "episode: 177 episode reward: -777.2928674275222 epsilon: 0.9725330437547093 avg reward (last 100): -792.5705267532194 episode loss:  10089.935\n",
      "episode: 178 episode reward: -1462.5248318296815 epsilon: 0.9724357904503338 avg reward (last 100): -805.5104682102435 episode loss:  15493.063\n",
      "episode: 179 episode reward: -451.1738670240759 epsilon: 0.9723385468712887 avg reward (last 100): -806.3296261358021 episode loss:  16217.531\n",
      "episode: 180 episode reward: -1619.758144582676 epsilon: 0.9722413130166017 avg reward (last 100): -800.0040899269808 episode loss:  76254.03\n",
      "episode: 181 episode reward: -254.00936052554607 epsilon: 0.9721440888853 avg reward (last 100): -801.0521079994044 episode loss:  148251.02\n",
      "episode: 182 episode reward: -404.9601317471868 epsilon: 0.9720468744764115 avg reward (last 100): -789.1515251080152 episode loss:  95286.15\n",
      "episode: 183 episode reward: -329.2499910060707 epsilon: 0.9719496697889638 avg reward (last 100): -780.2708814138242 episode loss:  42972.816\n",
      "episode: 184 episode reward: -218.01716067613197 epsilon: 0.9718524748219849 avg reward (last 100): -771.0180373470536 episode loss:  15704.343\n",
      "episode: 185 episode reward: -1108.2835201795642 epsilon: 0.9717552895745027 avg reward (last 100): -766.1272712704466 episode loss:  16219.782\n",
      "episode: 186 episode reward: -236.5316538797866 epsilon: 0.9716581140455453 avg reward (last 100): -766.8485256212822 episode loss:  39770.324\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
